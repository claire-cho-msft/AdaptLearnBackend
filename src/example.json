[
    {
        "Tutorial: Explore Azure OpenAI in Azure AI Foundry Models embeddings and document search": {
            "tag": "h1",
            "text": "",
            "html": "",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "In this article": {
            "tag": "h2",
            "text": "",
            "html": "\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "Prerequisites": {
            "tag": "h2",
            "text": "An Azure subscription - Create one for free An Azure OpenAI resource with the text-embedding-ada-002 (Version 2) model deployed. This model is currently only available in certain regions .  If you don't have a resource the process of creating one is documented in our resource deployment guide . Python 3.8 or later version The following Python libraries: openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken. Jupyter Notebooks",
            "html": "\n<ul>\n<li>An Azure subscription - <a data-linktype=\"external\" href=\"https://azure.microsoft.com/free/cognitive-services?azure-portal=true\">Create one for free</a></li>\n<li>An Azure OpenAI resource with the <strong>text-embedding-ada-002 (Version 2)</strong> model deployed. This model is currently only available in <a data-linktype=\"relative-path\" href=\"../concepts/models#model-summary-table-and-region-availability\">certain regions</a>.  If you don't have a resource the process of creating one is documented in our <a data-linktype=\"relative-path\" href=\"../how-to/create-resource\">resource deployment guide</a>.</li>\n<li><a data-linktype=\"external\" href=\"https://www.python.org/\" target=\"_blank\">Python 3.8 or later version</a></li>\n<li>The following Python libraries: openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken.</li>\n<li><a data-linktype=\"external\" href=\"https://jupyter.org/\">Jupyter Notebooks</a></li>\n</ul>\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "Set up": {
            "tag": "h2",
            "text": "Python libraries If you haven't already, you need to install the following libraries: OpenAI Python 1.x OpenAI Python 0.28.1 pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken Note The OpenAI Python library version 0.28.1 is deprecated. We recommend using 1.x . Consult our migration guide for information on moving from 0.28.1 to 1.x . pip install \"openai==0.28.1\" num2words matplotlib plotly scipy scikit-learn pandas tiktoken Alternatively, you can use our [requirements.txt file](https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/requirements.txt). Download the BillSum dataset BillSum is a dataset of United States Congressional and California state bills. For illustration purposes, we'll look only at the US bills. The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 characters in length. More information on the project and the original academic paper where this dataset is derived from can be found on the BillSum project's GitHub repository This tutorial uses the bill_sum_data.csv file that can be downloaded from our GitHub sample data . You can also download the sample data by running the following command on your local machine: curl \"https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-Samples/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\" --output bill_sum_data.csv Retrieve key and endpoint To successfully make a call against Azure OpenAI, you need an endpoint and a key . Variable name Value ENDPOINT The service endpoint can be found in the Keys & Endpoint section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the Deployments page in Azure AI Foundry portal. An example endpoint is: https://docs-test-001.openai.azure.com/ . API-KEY This value can be found in the Keys & Endpoint section when examining your resource from the Azure portal. You can use either KEY1 or KEY2 . Go to your resource in the Azure portal. The Keys & Endpoint section can be found in the Resource Management section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either KEY1 or KEY2 . Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption. Environment variables Create and assign persistent environment variables for your key and endpoint. Important Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see API keys with Azure Key Vault . For more information about AI services security, see Authenticate requests to Azure AI services . Command Line PowerShell Bash setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User') [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User') echo export AZURE_OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" >> /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"REPLACE_WITH_YOUR_ENDPOINT_HERE\" >> /etc/environment\n\nsource /etc/environment After setting the environment variables, you might need to close and reopen Jupyter notebooks or whatever IDE you're using in order for the environment variables to be accessible. While we strongly recommend using Jupyter Notebooks, if for some reason you can't you'll need to modify any code that is returning a pandas dataframe by using print(dataframe_name) rather than just calling the dataframe_name directly as is often done at the end of a code block. Run the following code in your preferred Python IDE: If you wish to view the Jupyter notebook that corresponds to this tutorial you can download the tutorial from our [samples repo](https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/embedding_billsum.ipynb).",
            "html": "\n<h3 id=\"python-libraries\">Python libraries</h3>\n<p>If you haven't already, you need to install the following libraries:</p>\n<div class=\"tabGroup\" id=\"tabgroup_1\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_1_python-new\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"python-new\" href=\"#tabpanel_1_python-new\" role=\"tab\" tabindex=\"0\">OpenAI Python 1.x</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_1_python\" data-linktype=\"self-bookmark\" data-tab=\"python\" href=\"#tabpanel_1_python\" role=\"tab\" tabindex=\"-1\">OpenAI Python 0.28.1</a>\n</li>\n</ul>\n<section data-tab=\"python-new\" id=\"tabpanel_1_python-new\" role=\"tabpanel\">\n<pre><code class=\"lang-console\">pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"python\" hidden=\"hidden\" id=\"tabpanel_1_python\" role=\"tabpanel\">\n<div class=\"NOTE\">\n<p>Note</p>\n<p>The OpenAI Python library version <code>0.28.1</code> is deprecated. We recommend using <code>1.x</code>. Consult our <a data-linktype=\"relative-path\" href=\"../how-to/migration\">migration guide</a> for information on moving from <code>0.28.1</code> to <code>1.x</code>.</p>\n</div>\n<pre><code class=\"lang-cmd\">pip install \"openai==0.28.1\" num2words matplotlib plotly scipy scikit-learn pandas tiktoken\n</code></pre>\n</section>\n</div>\nAlternatively, you can use our [requirements.txt file](https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/requirements.txt).\n<h3 id=\"download-the-billsum-dataset\">Download the BillSum dataset</h3>\n<p>BillSum is a dataset of United States Congressional and California state bills. For illustration purposes, we'll look only at the US bills. The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 characters in length. More information on the project and the original academic paper where this dataset is derived from can be found on the <a data-linktype=\"external\" href=\"https://github.com/FiscalNote/BillSum\">BillSum project's GitHub repository</a></p>\n<p>This tutorial uses the <code>bill_sum_data.csv</code> file that can be downloaded from our <a data-linktype=\"external\" href=\"https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\">GitHub sample data</a>.</p>\n<p>You can also download the sample data by running the following command on your local machine:</p>\n<pre><code class=\"lang-cmd\">curl \"https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-Samples/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\" --output bill_sum_data.csv\n</code></pre>\n<h3 id=\"retrieve-key-and-endpoint\">Retrieve key and endpoint</h3>\n<p>To successfully make a call against Azure OpenAI, you need an <strong>endpoint</strong> and a <strong>key</strong>.</p>\n<table>\n<thead>\n<tr>\n<th>Variable name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ENDPOINT</code></td>\n<td>The service endpoint can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the <strong>Deployments</strong> page in Azure AI Foundry portal. An example endpoint is: <code>https://docs-test-001.openai.azure.com/</code>.</td>\n</tr>\n<tr>\n<td><code>API-KEY</code></td>\n<td>This value can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. You can use either <code>KEY1</code> or <code>KEY2</code>.</td>\n</tr>\n</tbody>\n</table>\n<p>Go to your resource in the Azure portal. The <strong>Keys &amp; Endpoint</strong> section can be found in the <strong>Resource Management</strong> section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either <code>KEY1</code> or <code>KEY2</code>. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.</p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/quickstarts/endpoint.png#lightbox\">\n<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>\n</a>\n</span>\n</p>\n<h3 id=\"environment-variables\">Environment variables</h3>\n<p>Create and assign persistent environment variables for your key and endpoint.</p>\n<div class=\"IMPORTANT\">\n<p>Important</p>\n<p>Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see <a data-linktype=\"absolute-path\" href=\"/en-us/azure/key-vault/general/apps-api-keys-secrets\">API keys with Azure Key Vault</a>.</p>\n<p>For more information about AI services security, see <a data-linktype=\"absolute-path\" href=\"/en-us/azure/ai-services/authentication\">Authenticate requests to Azure AI services</a>.</p>\n</div>\n<div class=\"tabGroup\" id=\"tabgroup_2\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_2_command-line\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"command-line\" href=\"#tabpanel_2_command-line\" role=\"tab\" tabindex=\"0\">Command Line</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_2_powershell\" data-linktype=\"self-bookmark\" data-tab=\"powershell\" href=\"#tabpanel_2_powershell\" role=\"tab\" tabindex=\"-1\">PowerShell</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_2_bash\" data-linktype=\"self-bookmark\" data-tab=\"bash\" href=\"#tabpanel_2_bash\" role=\"tab\" tabindex=\"-1\">Bash</a>\n</li>\n</ul>\n<section data-tab=\"command-line\" id=\"tabpanel_2_command-line\" role=\"tabpanel\">\n<pre><code class=\"lang-CMD\">setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" \n</code></pre>\n<pre><code class=\"lang-CMD\">setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" \n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"powershell\" hidden=\"hidden\" id=\"tabpanel_2_powershell\" role=\"tabpanel\">\n<pre><code class=\"lang-powershell\">[System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')\n</code></pre>\n<pre><code class=\"lang-powershell\">[System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"bash\" hidden=\"hidden\" id=\"tabpanel_2_bash\" role=\"tabpanel\">\n<pre><code class=\"lang-Bash\">echo export AZURE_OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"REPLACE_WITH_YOUR_ENDPOINT_HERE\" &gt;&gt; /etc/environment\n\nsource /etc/environment\n</code></pre>\n</section>\n</div>\n<p>After setting the environment variables, you might need to close and reopen Jupyter notebooks or whatever IDE you're using in order for the environment variables to be accessible. While we strongly recommend using Jupyter Notebooks, if for some reason you can't you'll need to modify any code that is returning a pandas dataframe by using <code>print(dataframe_name)</code> rather than just calling the <code>dataframe_name</code> directly as is often done at the end of a code block.</p>\n<p>Run the following code in your preferred Python IDE:</p>\nIf you wish to view the Jupyter notebook that corresponds to this tutorial you can download the tutorial from our [samples repo](https://github.com/Azure-Samples/Azure-OpenAI-Docs-Samples/blob/main/Samples/Tutorials/Embeddings/embedding_billsum.ipynb).\n",
            "images": [
                "<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>"
            ],
            "img_alt": [
                "<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>"
            ],
            "tables": [
                "<table>\n<thead>\n<tr>\n<th>Variable name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ENDPOINT</code></td>\n<td>The service endpoint can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the <strong>Deployments</strong> page in Azure AI Foundry portal. An example endpoint is: <code>https://docs-test-001.openai.azure.com/</code>.</td>\n</tr>\n<tr>\n<td><code>API-KEY</code></td>\n<td>This value can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. You can use either <code>KEY1</code> or <code>KEY2</code>.</td>\n</tr>\n</tbody>\n</table>"
            ],
            "codes": [
                "<code class=\"lang-console\">pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken\n</code>",
                "<code>0.28.1</code>",
                "<code>1.x</code>",
                "<code>0.28.1</code>",
                "<code>1.x</code>",
                "<code class=\"lang-cmd\">pip install \"openai==0.28.1\" num2words matplotlib plotly scipy scikit-learn pandas tiktoken\n</code>",
                "<code>bill_sum_data.csv</code>",
                "<code class=\"lang-cmd\">curl \"https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-Samples/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\" --output bill_sum_data.csv\n</code>",
                "<code>ENDPOINT</code>",
                "<code>https://docs-test-001.openai.azure.com/</code>",
                "<code>API-KEY</code>",
                "<code>KEY1</code>",
                "<code>KEY2</code>",
                "<code>KEY1</code>",
                "<code>KEY2</code>",
                "<code class=\"lang-CMD\">setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" \n</code>",
                "<code class=\"lang-CMD\">setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" \n</code>",
                "<code class=\"lang-powershell\">[System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')\n</code>",
                "<code class=\"lang-powershell\">[System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')\n</code>",
                "<code class=\"lang-Bash\">echo export AZURE_OPENAI_API_KEY=\"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"REPLACE_WITH_YOUR_ENDPOINT_HERE\" &gt;&gt; /etc/environment\n\nsource /etc/environment\n</code>",
                "<code>print(dataframe_name)</code>",
                "<code>dataframe_name</code>"
            ]
        }
    },
    {
        "Import libraries": {
            "tag": "h2",
            "text": "OpenAI Python 1.x OpenAI Python 0.28.1 import os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nimport tiktoken\nfrom openai import AzureOpenAI import openai\nimport os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\nimport tiktoken\n\nAPI_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") \nRESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n\nopenai.api_type = \"azure\"\nopenai.api_key = API_KEY\nopenai.api_base = RESOURCE_ENDPOINT\nopenai.api_version = \"2024-10-21\"\n\nurl = openai.api_base + \"/openai/deployments?api-version=2024-10-21\" \n\nr = requests.get(url, headers={\"api-key\": API_KEY})\n\nprint(r.text) {\n  \"data\": [\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-embedding-ada-002\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-embedding-ada-002\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572678,\n      \"updated_at\": 1657572678,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"code-cushman-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"code-cushman-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572712,\n      \"updated_at\": 1657572712,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-doc-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-doc-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1668620345,\n      \"updated_at\": 1668620345,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-query-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-query-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1669048765,\n      \"updated_at\": 1669048765,\n      \"object\": \"deployment\"\n    }\n  ],\n  \"object\": \"list\"\n} The output of this command will vary based on the number and type of models you've deployed. In this case, we need to confirm that we have an entry for text-embedding-ada-002 . If you find that you're missing this model, you'll need to deploy the model to your resource before proceeding. Now we need to read our csv file and create a pandas DataFrame. After the initial DataFrame is created, we can view the contents of the table by running df . df=pd.read_csv(os.path.join(os.getcwd(),'bill_sum_data.csv')) # This assumes that you have placed the bill_sum_data.csv in the same directory you are running Jupyter Notebooks\ndf Output: The initial table has more columns than we need we'll create a new smaller DataFrame called df_bills which will contain only the columns for text , summary , and title . df_bills = df[['text', 'summary', 'title']]\ndf_bills Output: Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization. pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n\n# s is input text\ndef normalize_text(s, sep_token = \" \\n \"):\n    s = re.sub(r'\\s+',  ' ', s).strip()\n    s = re.sub(r\". ,\",\"\",s)\n    # remove all instances of multiple spaces\n    s = s.replace(\"..\",\".\")\n    s = s.replace(\". .\",\".\")\n    s = s.replace(\"\\n\", \"\")\n    s = s.strip()\n    \n    return s\n\ndf_bills['text']= df_bills[\"text\"].apply(lambda x : normalize_text(x)) Now we need to remove any bills that are too long for the token limit (8192 tokens). tokenizer = tiktoken.get_encoding(\"cl100k_base\")\ndf_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\ndf_bills = df_bills[df_bills.n_tokens<8192]\nlen(df_bills) 20 Note In this case all bills are under the embedding model input token limit, but you can use the technique above to remove entries that would otherwise cause embedding to fail. When faced with content that exceeds the embedding limit, you can also chunk the content into smaller pieces and then embed those one at a time. We'll once again examine df_bills . df_bills Output: To understand the n_tokens column a little more as well how text ultimately is tokenized, it can be helpful to run the following code: sample_encode = tokenizer.encode(df_bills.text[0]) \ndecode = tokenizer.decode_tokens_bytes(sample_encode)\ndecode For our docs we're intentionally truncating the output, but running this command in your environment will return the full text from index zero tokenized into chunks. You can see that in some cases an entire word is represented with a single token whereas in others parts of words are split across multiple tokens. [b'SECTION',\n b' ',\n b'1',\n b'.',\n b' SHORT',\n b' TITLE',\n b'.',\n b' This',\n b' Act',\n b' may',\n b' be',\n b' cited',\n b' as',\n b' the',\n b' ``',\n b'National',\n b' Science',\n b' Education',\n b' Tax',\n b' In',\n b'cent',\n b'ive',\n b' for',\n b' Businesses',\n b' Act',\n b' of',\n b' ',\n b'200',\n b'7',\n b\"''.\",\n b' SEC',\n b'.',\n b' ',\n b'2',\n b'.',\n b' C',\n b'RED',\n b'ITS',\n b' FOR',\n b' CERT',\n b'AIN',\n b' CONTRIBUT',\n b'IONS',\n b' BEN',\n b'EF',\n b'IT',\n b'ING',\n b' SC', If you then check the length of the decode variable, you'll find it matches the first number in the n_tokens column. len(decode) 1466 Now that we understand more about how tokenization works we can move on to embedding. It's important to note, that we haven't actually tokenized the documents yet. The n_tokens column is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192. When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples above and then convert the tokens to a series of floating point numbers that will be accessible via vector search. These embeddings can be stored locally or in an Azure Database to support Vector Search . As a result, each bill will have its own corresponding embedding vector in the new ada_v2 column on the right side of the DataFrame. In the example below we're calling the embedding model once per every item that we want to embed. When working with large embedding projects you can alternatively pass the model an array of inputs to embed rather than one input at a time. When you pass the model an array of inputs the max number of input items per call to the embedding endpoint is 2048. OpenAI Python 1.x OpenAI Python 0.28.1 client = AzureOpenAI(\n  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n  api_version = \"2024-02-01\",\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n)\n\ndef generate_embeddings(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002')) # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002')) # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model df_bills Output: As we run the search code block below, we'll embed the search query \"Can I get information on cable company tax revenue?\" with the same text-embedding-ada-002 (Version 2) model. Next we'll find the closest bill embedding to the newly embedded text from our query ranked by cosine similarity . OpenAI Python 1.x OpenAI Python 0.28.1 def cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef get_embedding(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndef search_docs(df, user_query, top_n=4, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        model=\"text-embedding-ada-002\" # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4) # search through the reviews for a specific product\ndef search_docs(df, user_query, top_n=3, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        engine=\"text-embedding-ada-002\" # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4) Output : Finally, we'll show the top result from document search based on user query against the entire knowledge base. This returns the top result of the \"Taxpayer's Right to View Act of 1993\". This document has a cosine similarity score of 0.76 between the query and the document: res[\"summary\"][9] \"Taxpayer's Right to View Act of 1993 - Amends the Communications Act of 1934 to prohibit a cable operator from assessing separate charges for any video programming of a sporting, theatrical, or other entertainment event if that event is performed at a facility constructed, renovated, or maintained with tax revenues or by an organization that receives public financial support. Authorizes the Federal Communications Commission and local franchising authorities to make determinations concerning the applicability of such prohibition. Sets forth conditions under which a facility is considered to have been constructed, maintained, or renovated with tax revenues. Considers events performed by nonprofit or public organizations that receive tax subsidies to be subject to this Act if the event is sponsored by, or includes the participation of a team that is part of, a tax exempt organization.\"",
            "html": "\n<div class=\"tabGroup\" id=\"tabgroup_3\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_3_python-new\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"python-new\" href=\"#tabpanel_3_python-new\" role=\"tab\" tabindex=\"0\">OpenAI Python 1.x</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_3_python\" data-linktype=\"self-bookmark\" data-tab=\"python\" href=\"#tabpanel_3_python\" role=\"tab\" tabindex=\"-1\">OpenAI Python 0.28.1</a>\n</li>\n</ul>\n<section data-tab=\"python-new\" id=\"tabpanel_3_python-new\" role=\"tabpanel\">\n<pre><code class=\"lang-python\">import os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nimport tiktoken\nfrom openai import AzureOpenAI\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"python\" hidden=\"hidden\" id=\"tabpanel_3_python\" role=\"tabpanel\">\n<pre><code class=\"lang-python\">import openai\nimport os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\nimport tiktoken\n\nAPI_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") \nRESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n\nopenai.api_type = \"azure\"\nopenai.api_key = API_KEY\nopenai.api_base = RESOURCE_ENDPOINT\nopenai.api_version = \"2024-10-21\"\n\nurl = openai.api_base + \"/openai/deployments?api-version=2024-10-21\" \n\nr = requests.get(url, headers={\"api-key\": API_KEY})\n\nprint(r.text)\n</code></pre>\n<pre><code class=\"lang-output\">{\n  \"data\": [\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-embedding-ada-002\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-embedding-ada-002\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572678,\n      \"updated_at\": 1657572678,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"code-cushman-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"code-cushman-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572712,\n      \"updated_at\": 1657572712,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-doc-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-doc-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1668620345,\n      \"updated_at\": 1668620345,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-query-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-query-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1669048765,\n      \"updated_at\": 1669048765,\n      \"object\": \"deployment\"\n    }\n  ],\n  \"object\": \"list\"\n}\n</code></pre>\n<p>The output of this command will vary based on the number and type of models you've deployed. In this case, we need to confirm that we have an entry for <strong>text-embedding-ada-002</strong>. If you find that you're missing this model, you'll need to <a data-linktype=\"relative-path\" href=\"../how-to/create-resource#deploy-a-model\">deploy the model</a> to your resource before proceeding.</p>\n</section>\n</div>\n<p>Now we need to read our csv file and create a pandas DataFrame. After the initial DataFrame is created, we can view the contents of the table by running <code>df</code>.</p>\n<pre><code class=\"lang-python\">df=pd.read_csv(os.path.join(os.getcwd(),'bill_sum_data.csv')) # This assumes that you have placed the bill_sum_data.csv in the same directory you are running Jupyter Notebooks\ndf\n</code></pre>\n<p><strong>Output:</strong></p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/initial-dataframe.png#lightbox\">\n<img alt=\"Screenshot of the initial DataFrame table results from the csv file.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-dataframe.png\"/>\n</a>\n</span>\n</p>\n<p>The initial table has more columns than we need we'll create a new smaller DataFrame called <code>df_bills</code> which will contain only the columns for <code>text</code>, <code>summary</code>, and <code>title</code>.</p>\n<pre><code class=\"lang-python\">df_bills = df[['text', 'summary', 'title']]\ndf_bills\n</code></pre>\n<p><strong>Output:</strong></p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/cleanup-dataframe.png#lightbox\">\n<img alt=\"Screenshot of the smaller DataFrame table results with only text, summary and title columns displayed.\" data-linktype=\"relative-path\" src=\"../media/tutorials/cleanup-dataframe.png\"/>\n</a>\n</span>\n</p>\n<p>Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization.</p>\n<pre><code class=\"lang-python\">pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n\n# s is input text\ndef normalize_text(s, sep_token = \" \\n \"):\n    s = re.sub(r'\\s+',  ' ', s).strip()\n    s = re.sub(r\". ,\",\"\",s)\n    # remove all instances of multiple spaces\n    s = s.replace(\"..\",\".\")\n    s = s.replace(\". .\",\".\")\n    s = s.replace(\"\\n\", \"\")\n    s = s.strip()\n    \n    return s\n\ndf_bills['text']= df_bills[\"text\"].apply(lambda x : normalize_text(x))\n</code></pre>\n<p>Now we need to remove any bills that are too long for the token limit (8192 tokens).</p>\n<pre><code class=\"lang-python\">tokenizer = tiktoken.get_encoding(\"cl100k_base\")\ndf_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\ndf_bills = df_bills[df_bills.n_tokens&lt;8192]\nlen(df_bills)\n</code></pre>\n<pre><code class=\"lang-output\">20\n</code></pre>\n<div class=\"NOTE\">\n<p>Note</p>\n<p>In this case all bills are under the embedding model input token limit, but you can use the technique above to remove entries that would otherwise cause embedding to fail. When faced with content that exceeds the embedding limit, you can also chunk the content into smaller pieces and then embed those one at a time.</p>\n</div>\n<p>We'll once again examine <strong>df_bills</strong>.</p>\n<pre><code class=\"lang-python\">df_bills\n</code></pre>\n<p><strong>Output:</strong></p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/tokens-dataframe.png#lightbox\">\n<img alt=\"Screenshot of the DataFrame with a new column called n_tokens.\" data-linktype=\"relative-path\" src=\"../media/tutorials/tokens-dataframe.png\"/>\n</a>\n</span>\n</p>\n<p>To understand the n_tokens column a little more as well how text ultimately is tokenized, it can be helpful to run the following code:</p>\n<pre><code class=\"lang-python\">sample_encode = tokenizer.encode(df_bills.text[0]) \ndecode = tokenizer.decode_tokens_bytes(sample_encode)\ndecode\n</code></pre>\n<p>For our docs we're intentionally truncating the output, but running this command in your environment will return the full text from index zero tokenized into chunks. You can see that in some cases an entire word is represented with a single token whereas in others parts of words are split across multiple tokens.</p>\n<pre><code class=\"lang-output\">[b'SECTION',\n b' ',\n b'1',\n b'.',\n b' SHORT',\n b' TITLE',\n b'.',\n b' This',\n b' Act',\n b' may',\n b' be',\n b' cited',\n b' as',\n b' the',\n b' ``',\n b'National',\n b' Science',\n b' Education',\n b' Tax',\n b' In',\n b'cent',\n b'ive',\n b' for',\n b' Businesses',\n b' Act',\n b' of',\n b' ',\n b'200',\n b'7',\n b\"''.\",\n b' SEC',\n b'.',\n b' ',\n b'2',\n b'.',\n b' C',\n b'RED',\n b'ITS',\n b' FOR',\n b' CERT',\n b'AIN',\n b' CONTRIBUT',\n b'IONS',\n b' BEN',\n b'EF',\n b'IT',\n b'ING',\n b' SC',\n</code></pre>\n<p>If you then check the length of the <code>decode</code> variable, you'll find it matches the first number in the n_tokens column.</p>\n<pre><code class=\"lang-python\">len(decode)\n</code></pre>\n<pre><code class=\"lang-output\">1466\n</code></pre>\n<p>Now that we understand more about how tokenization works we can move on to embedding. It's important to note, that we haven't actually tokenized the documents yet. The <code>n_tokens</code> column is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192. When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples above and then convert the tokens to a series of floating point numbers that will be accessible via vector search. These embeddings can be stored locally or in an <a data-linktype=\"absolute-path\" href=\"/en-us/azure/cosmos-db/mongodb/vcore/vector-search\">Azure Database to support Vector Search</a>. As a result, each bill will have its own corresponding embedding vector in the new <code>ada_v2</code> column on the right side of the DataFrame.</p>\n<p>In the example below we're calling the embedding model once per every item that we want to embed. When working with large embedding projects you can alternatively pass the model an array of inputs to embed rather than one input at a time. When you pass the model an array of inputs the max number of input items per call to the embedding endpoint is 2048.</p>\n<div class=\"tabGroup\" id=\"tabgroup_4\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_4_python-new\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"python-new\" href=\"#tabpanel_4_python-new\" role=\"tab\" tabindex=\"0\">OpenAI Python 1.x</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_4_python\" data-linktype=\"self-bookmark\" data-tab=\"python\" href=\"#tabpanel_4_python\" role=\"tab\" tabindex=\"-1\">OpenAI Python 0.28.1</a>\n</li>\n</ul>\n<section data-tab=\"python-new\" id=\"tabpanel_4_python-new\" role=\"tabpanel\">\n<pre><code class=\"lang-python\">client = AzureOpenAI(\n  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n  api_version = \"2024-02-01\",\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n)\n\ndef generate_embeddings(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002')) # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"python\" hidden=\"hidden\" id=\"tabpanel_4_python\" role=\"tabpanel\">\n<pre><code class=\"lang-python\">df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002')) # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n</code></pre>\n</section>\n</div>\n<pre><code class=\"lang-python\">df_bills\n</code></pre>\n<p><strong>Output:</strong></p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/embed-text-documents.png#lightbox\">\n<img alt=\"Screenshot of the formatted results from df_bills command.\" data-linktype=\"relative-path\" src=\"../media/tutorials/embed-text-documents.png\"/>\n</a>\n</span>\n</p>\n<p>As we run the search code block below, we'll embed the search query <em>\"Can I get information on cable company tax revenue?\"</em> with the same <strong>text-embedding-ada-002 (Version 2)</strong> model. Next we'll find the closest bill embedding to the newly embedded text from our query ranked by <a data-linktype=\"relative-path\" href=\"../concepts/understand-embeddings\">cosine similarity</a>.</p>\n<div class=\"tabGroup\" id=\"tabgroup_5\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_5_python-new\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"python-new\" href=\"#tabpanel_5_python-new\" role=\"tab\" tabindex=\"0\">OpenAI Python 1.x</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_5_python\" data-linktype=\"self-bookmark\" data-tab=\"python\" href=\"#tabpanel_5_python\" role=\"tab\" tabindex=\"-1\">OpenAI Python 0.28.1</a>\n</li>\n</ul>\n<section data-tab=\"python-new\" id=\"tabpanel_5_python-new\" role=\"tabpanel\">\n<pre><code class=\"lang-python\">def cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef get_embedding(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndef search_docs(df, user_query, top_n=4, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        model=\"text-embedding-ada-002\" # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"python\" hidden=\"hidden\" id=\"tabpanel_5_python\" role=\"tabpanel\">\n<pre><code class=\"lang-python\"># search through the reviews for a specific product\ndef search_docs(df, user_query, top_n=3, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        engine=\"text-embedding-ada-002\" # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)\n</code></pre>\n</section>\n</div>\n<p><strong>Output</strong>:</p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/query-result.png#lightbox\">\n<img alt=\"Screenshot of the formatted results of res once the search query has been run.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result.png\"/>\n</a>\n</span>\n</p>\n<p>Finally, we'll show the top result from document search based on user query against the entire knowledge base. This returns the top result of the \"Taxpayer's Right to View Act of 1993\". This document has a cosine similarity score of 0.76 between the query and the document:</p>\n<pre><code class=\"lang-python\">res[\"summary\"][9]\n</code></pre>\n<pre><code class=\"lang-output\">\"Taxpayer's Right to View Act of 1993 - Amends the Communications Act of 1934 to prohibit a cable operator from assessing separate charges for any video programming of a sporting, theatrical, or other entertainment event if that event is performed at a facility constructed, renovated, or maintained with tax revenues or by an organization that receives public financial support. Authorizes the Federal Communications Commission and local franchising authorities to make determinations concerning the applicability of such prohibition. Sets forth conditions under which a facility is considered to have been constructed, maintained, or renovated with tax revenues. Considers events performed by nonprofit or public organizations that receive tax subsidies to be subject to this Act if the event is sponsored by, or includes the participation of a team that is part of, a tax exempt organization.\"\n</code></pre>\n",
            "images": [
                "<img alt=\"Screenshot of the initial DataFrame table results from the csv file.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-dataframe.png\"/>",
                "<img alt=\"Screenshot of the smaller DataFrame table results with only text, summary and title columns displayed.\" data-linktype=\"relative-path\" src=\"../media/tutorials/cleanup-dataframe.png\"/>",
                "<img alt=\"Screenshot of the DataFrame with a new column called n_tokens.\" data-linktype=\"relative-path\" src=\"../media/tutorials/tokens-dataframe.png\"/>",
                "<img alt=\"Screenshot of the formatted results from df_bills command.\" data-linktype=\"relative-path\" src=\"../media/tutorials/embed-text-documents.png\"/>",
                "<img alt=\"Screenshot of the formatted results of res once the search query has been run.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result.png\"/>"
            ],
            "img_alt": [
                "<img alt=\"Screenshot of the initial DataFrame table results from the csv file.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-dataframe.png\"/>",
                "<img alt=\"Screenshot of the smaller DataFrame table results with only text, summary and title columns displayed.\" data-linktype=\"relative-path\" src=\"../media/tutorials/cleanup-dataframe.png\"/>",
                "<img alt=\"Screenshot of the DataFrame with a new column called n_tokens.\" data-linktype=\"relative-path\" src=\"../media/tutorials/tokens-dataframe.png\"/>",
                "<img alt=\"Screenshot of the formatted results from df_bills command.\" data-linktype=\"relative-path\" src=\"../media/tutorials/embed-text-documents.png\"/>",
                "<img alt=\"Screenshot of the formatted results of res once the search query has been run.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result.png\"/>"
            ],
            "tables": [],
            "codes": [
                "<code class=\"lang-python\">import os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nimport tiktoken\nfrom openai import AzureOpenAI\n</code>",
                "<code class=\"lang-python\">import openai\nimport os\nimport re\nimport requests\nimport sys\nfrom num2words import num2words\nimport os\nimport pandas as pd\nimport numpy as np\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\nimport tiktoken\n\nAPI_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") \nRESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n\nopenai.api_type = \"azure\"\nopenai.api_key = API_KEY\nopenai.api_base = RESOURCE_ENDPOINT\nopenai.api_version = \"2024-10-21\"\n\nurl = openai.api_base + \"/openai/deployments?api-version=2024-10-21\" \n\nr = requests.get(url, headers={\"api-key\": API_KEY})\n\nprint(r.text)\n</code>",
                "<code class=\"lang-output\">{\n  \"data\": [\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-embedding-ada-002\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-embedding-ada-002\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572678,\n      \"updated_at\": 1657572678,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"code-cushman-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"code-cushman-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1657572712,\n      \"updated_at\": 1657572712,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-doc-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-doc-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1668620345,\n      \"updated_at\": 1668620345,\n      \"object\": \"deployment\"\n    },\n    {\n      \"scale_settings\": {\n        \"scale_type\": \"standard\"\n      },\n      \"model\": \"text-search-curie-query-001\",\n      \"owner\": \"organization-owner\",\n      \"id\": \"text-search-curie-query-001\",\n      \"status\": \"succeeded\",\n      \"created_at\": 1669048765,\n      \"updated_at\": 1669048765,\n      \"object\": \"deployment\"\n    }\n  ],\n  \"object\": \"list\"\n}\n</code>",
                "<code>df</code>",
                "<code class=\"lang-python\">df=pd.read_csv(os.path.join(os.getcwd(),'bill_sum_data.csv')) # This assumes that you have placed the bill_sum_data.csv in the same directory you are running Jupyter Notebooks\ndf\n</code>",
                "<code>df_bills</code>",
                "<code>text</code>",
                "<code>summary</code>",
                "<code>title</code>",
                "<code class=\"lang-python\">df_bills = df[['text', 'summary', 'title']]\ndf_bills\n</code>",
                "<code class=\"lang-python\">pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n\n# s is input text\ndef normalize_text(s, sep_token = \" \\n \"):\n    s = re.sub(r'\\s+',  ' ', s).strip()\n    s = re.sub(r\". ,\",\"\",s)\n    # remove all instances of multiple spaces\n    s = s.replace(\"..\",\".\")\n    s = s.replace(\". .\",\".\")\n    s = s.replace(\"\\n\", \"\")\n    s = s.strip()\n    \n    return s\n\ndf_bills['text']= df_bills[\"text\"].apply(lambda x : normalize_text(x))\n</code>",
                "<code class=\"lang-python\">tokenizer = tiktoken.get_encoding(\"cl100k_base\")\ndf_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\ndf_bills = df_bills[df_bills.n_tokens&lt;8192]\nlen(df_bills)\n</code>",
                "<code class=\"lang-output\">20\n</code>",
                "<code class=\"lang-python\">df_bills\n</code>",
                "<code class=\"lang-python\">sample_encode = tokenizer.encode(df_bills.text[0]) \ndecode = tokenizer.decode_tokens_bytes(sample_encode)\ndecode\n</code>",
                "<code class=\"lang-output\">[b'SECTION',\n b' ',\n b'1',\n b'.',\n b' SHORT',\n b' TITLE',\n b'.',\n b' This',\n b' Act',\n b' may',\n b' be',\n b' cited',\n b' as',\n b' the',\n b' ``',\n b'National',\n b' Science',\n b' Education',\n b' Tax',\n b' In',\n b'cent',\n b'ive',\n b' for',\n b' Businesses',\n b' Act',\n b' of',\n b' ',\n b'200',\n b'7',\n b\"''.\",\n b' SEC',\n b'.',\n b' ',\n b'2',\n b'.',\n b' C',\n b'RED',\n b'ITS',\n b' FOR',\n b' CERT',\n b'AIN',\n b' CONTRIBUT',\n b'IONS',\n b' BEN',\n b'EF',\n b'IT',\n b'ING',\n b' SC',\n</code>",
                "<code>decode</code>",
                "<code class=\"lang-python\">len(decode)\n</code>",
                "<code class=\"lang-output\">1466\n</code>",
                "<code>n_tokens</code>",
                "<code>ada_v2</code>",
                "<code class=\"lang-python\">client = AzureOpenAI(\n  api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n  api_version = \"2024-02-01\",\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n)\n\ndef generate_embeddings(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002')) # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n</code>",
                "<code class=\"lang-python\">df_bills['ada_v2'] = df_bills[\"text\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002')) # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n</code>",
                "<code class=\"lang-python\">df_bills\n</code>",
                "<code class=\"lang-python\">def cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\ndef get_embedding(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndef search_docs(df, user_query, top_n=4, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        model=\"text-embedding-ada-002\" # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)\n</code>",
                "<code class=\"lang-python\"># search through the reviews for a specific product\ndef search_docs(df, user_query, top_n=3, to_print=True):\n    embedding = get_embedding(\n        user_query,\n        engine=\"text-embedding-ada-002\" # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n    )\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\n\n    res = (\n        df.sort_values(\"similarities\", ascending=False)\n        .head(top_n)\n    )\n    if to_print:\n        display(res)\n    return res\n\n\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)\n</code>",
                "<code class=\"lang-python\">res[\"summary\"][9]\n</code>",
                "<code class=\"lang-output\">\"Taxpayer's Right to View Act of 1993 - Amends the Communications Act of 1934 to prohibit a cable operator from assessing separate charges for any video programming of a sporting, theatrical, or other entertainment event if that event is performed at a facility constructed, renovated, or maintained with tax revenues or by an organization that receives public financial support. Authorizes the Federal Communications Commission and local franchising authorities to make determinations concerning the applicability of such prohibition. Sets forth conditions under which a facility is considered to have been constructed, maintained, or renovated with tax revenues. Considers events performed by nonprofit or public organizations that receive tax subsidies to be subject to this Act if the event is sponsored by, or includes the participation of a team that is part of, a tax exempt organization.\"\n</code>"
            ]
        }
    },
    {
        "Prerequisites": {
            "tag": "h2",
            "text": "An Azure subscription - Create one for free An Azure OpenAI resource with the text-embedding-ada-002 (Version 2) model deployed. This model is currently only available in certain regions .  If you don't have a resource\nthe process of creating one is documented in our resource deployment guide . PowerShell 7.4 Note Many examples in this tutorial re-use variables from step-to-step. Keep the same terminal session\nopen throughout. If variables you set in a previous step are lost due to closing the terminal,\nyou must begin again from the start. Retrieve key and endpoint To successfully make a call against Azure OpenAI, you need an endpoint and a key . Variable name Value ENDPOINT The service endpoint can be found in the Keys & Endpoint section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the Deployments page in Azure AI Foundry portal. An example endpoint is: https://docs-test-001.openai.azure.com/ . API-KEY This value can be found in the Keys & Endpoint section when examining your resource from the Azure portal. You can use either KEY1 or KEY2 . Go to your resource in the Azure portal. The Keys & Endpoint section can be found in the Resource Management section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either KEY1 or KEY2 . Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption. Environment variables Create and assign persistent environment variables for your key and endpoint. Important Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see API keys with Azure Key Vault . For more information about AI services security, see Authenticate requests to Azure AI services . Command Line PowerShell Bash setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" $Env:AZURE_OPENAI_API_KEY = '<YOUR_KEY_VALUE>'\n$Env:AZURE_OPENAI_ENDPOINT = '<YOUR_ENDPOINT>'\n$Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT = '<YOUR_DEPLOYMENT_NAME>' echo export AZURE_OPENAI_API_KEY=\"<YOUR_KEY_VALUE>\" >> /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"<YOUR_ENDPOINT>\" >> /etc/environment\necho export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=\"<YOUR_DEPLOYMENT_NAME>\" >> /etc/environment\nsource /etc/environment For this tutorial, we use the PowerShell 7.4 reference documentation as a well-known and safe\nsample dataset. As an alternative, you might choose to explore the Microsoft Research tools sample datasets. Create a folder where you would like to store your project. Set your location to the project\nfolder. Download the dataset to your local machine using the Invoke-WebRequest command and then\nexpand the archive. Last, set your location to the subfolder containing reference information for\nPowerShell version 7.4. New-Item '<FILE-PATH-TO-YOUR-PROJECT>' -Type Directory\nSet-Location '<FILE-PATH-TO-YOUR-PROJECT>'\n\n$DocsUri = 'https://github.com/MicrosoftDocs/PowerShell-Docs/archive/refs/heads/main.zip'\nInvoke-WebRequest $DocsUri -OutFile './PSDocs.zip'\n\nExpand-Archive './PSDocs.zip'\nSet-Location './PSDocs/PowerShell-Docs-main/reference/7.4/' We're working with a large amount of data in this tutorial, so we use a .NET data table object for\nefficient performance. The datatable has columns title , content , prep , uri , file , and vectors . The title column is the primary key . In the next step, we load the content of each markdown file into the data table. We also use\nPowerShell -match operator to capture known lines of text title: and online version: , and\nstore them in distinct columns. Some of the files don't contain the metadata lines of text, but\nsince they're overview pages and not detailed reference docs, we exclude them from the datatable. # make sure your location is the project subfolder\n\n$DataTable = New-Object System.Data.DataTable\n\n'title', 'content', 'prep', 'uri', 'file', 'vectors' | ForEach-Object {\n    $DataTable.Columns.Add($_)\n} | Out-Null\n$DataTable.PrimaryKey = $DataTable.Columns['title']\n\n$md = Get-ChildItem -Path . -Include *.md -Recurse\n\n$md | ForEach-Object {\n    $file       = $_.FullName\n    $content    = Get-Content $file\n    $title      = $content | Where-Object { $_ -match 'title: ' }\n    $uri        = $content | Where-Object { $_ -match 'online version: ' }\n    if ($title -and $uri) {\n        $row                = $DataTable.NewRow()\n        $row.title          = $title.ToString().Replace('title: ', '')\n        $row.content        = $content | Out-String\n        $row.prep           = '' # use later in the tutorial\n        $row.uri            = $uri.ToString().Replace('online version: ', '')\n        $row.file           = $file\n        $row.vectors        = '' # use later in the tutorial\n        $Datatable.rows.add($row)\n    }\n} View the data using the out-gridview command (not available in Cloud Shell). $Datatable | out-gridview Output: Next perform some light data cleaning by removing extra characters, empty space, and other document\nnotations, to prepare the data for tokenization. The sample function Invoke-DocPrep demonstrates\nhow to use the PowerShell -replace operator to iterate through a list of characters you would like\nto remove from the content. # sample demonstrates how to use `-replace` to remove characters from text content\nfunction Invoke-DocPrep {\nparam(\n    [Parameter(Mandatory = $true, ValueFromPipeline = $true)]\n    [string]$content\n)\n    # tab, line breaks, empty space\n    $replace = @('\\t','\\r\\n','\\n','\\r')\n    # non-UTF8 characters\n    $replace += @('[^\\x00-\\x7F]')\n    # html\n    $replace += @('<table>','</table>','<tr>','</tr>','<td>','</td>')\n    $replace += @('<ul>','</ul>','<li>','</li>')\n    $replace += @('<p>','</p>','<br>')\n    # docs\n    $replace += @('\\*\\*IMPORTANT:\\*\\*','\\*\\*NOTE:\\*\\*')\n    $replace += @('<!','no-loc ','text=')\n    $replace += @('<--','-->','---','--',':::')\n    # markdown\n    $replace += @('###','##','#','```')\n    $replace | ForEach-Object {\n        $content = $content -replace $_, ' ' -replace '  ',' '\n    }\n    return $content\n} After you create the Invoke-DocPrep function, use the ForEach-Object command to store prepared\ncontent in the prep column, for all rows in the datatable. We're using a new column so the\noriginal formatting is available if we would like to retrieve it later. $Datatable.rows | ForEach-Object { $_.prep = Invoke-DocPrep $_.content } View the datatable again to see the change. $Datatable | out-gridview When we pass the documents to the embeddings model, it encodes the documents into tokens and then\nreturns a series of floating point numbers to use in a cosine similarity search. These\nembeddings can be stored locally or in a service such as Vector Search in Azure AI Search .\nEach document has its own corresponding embedding vector in the new vectors column. The next example loops through each row in the datatable, retrieves the vectors for the\npreprocessed content, and stores them to the vectors column. The OpenAI service throttles\nfrequent requests, so the example includes an exponential back-off as suggested by the documentation . After the script completes, each row should have a comma-delimited list of 1536 vectors for each\ndocument. If an error occurs and the status code is 400 , the file path, title, and error code\nare added to a variable named $errorDocs for troubleshooting. The most common error occurs when\nthe token count is more than the prompt limit for the model. # Azure OpenAI metadata variables\n$openai = @{\n    api_key     = $Env:AZURE_OPENAI_API_KEY \n    api_base    = $Env:AZURE_OPENAI_ENDPOINT # should look like 'https://<YOUR_RESOURCE_NAME>.openai.azure.com/'\n    api_version = '2024-02-01' # may change in the future\n    name        = $Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT # custom name you chose for your deployment\n}\n\n$headers = [ordered]@{\n    'api-key' = $openai.api_key\n}\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$Datatable | ForEach-Object {\n    $doc = $_\n\n    $body = [ordered]@{\n        input = $doc.prep\n    } | ConvertTo-Json\n\n    $retryCount = 0\n    $maxRetries = 10\n    $delay      = 1\n    $docErrors = @()\n\n    do {\n        try {\n            $params = @{\n                Uri         = $url\n                Headers     = $headers\n                Body        = $body\n                Method      = 'Post'\n                ContentType = 'application/json'\n            }\n            $response = Invoke-RestMethod @params\n            $Datatable.rows.find($doc.title).vectors = $response.data.embedding -join ','\n            break\n        } catch {\n            if ($_.Exception.Response.StatusCode -eq 429) {\n                $retryCount++\n                [int]$retryAfter = $_.Exception.Response.Headers |\n                    Where-Object key -eq 'Retry-After' |\n                    Select-Object -ExpandProperty Value\n\n                # Use delay from error header\n                if ($delay -lt $retryAfter) { $delay = $retryAfter++ }\n                Start-Sleep -Seconds $delay\n                # Exponential back-off\n                $delay = [math]::min($delay * 1.5, 300)\n            } elseif ($_.Exception.Response.StatusCode -eq 400) {\n                if ($docErrors.file -notcontains $doc.file) {\n                    $docErrors += [ordered]@{\n                        error   = $_.exception.ErrorDetails.Message | ForEach-Object error | ForEach-Object message\n                        file    = $doc.file\n                        title   = $doc.title\n                    }\n                }\n            } else {\n                throw\n            }\n        }\n    } while ($retryCount -lt $maxRetries)\n}\nif (0 -lt $docErrors.count) {\n    Write-Host \"$($docErrors.count) documents encountered known errors such as too many tokens.`nReview the `$docErrors variable for details.\"\n} You now have a local in-memory database table of PowerShell 7.4 reference docs. Based on a search string, we need to calculate another set of vectors so PowerShell can rank each\ndocument by similarity. In the next example, vectors are retrieved for the search string get a list of running processes . $searchText = \"get a list of running processes\"\n\n$body = [ordered]@{\n    input = $searchText\n} | ConvertTo-Json\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$params = @{\n    Uri         = $url\n    Headers     = $headers\n    Body        = $body\n    Method      = 'Post'\n    ContentType = 'application/json'\n}\n$response = Invoke-RestMethod @params\n$searchVectors = $response.data.embedding -join ',' Finally, the next sample function, which borrows an example from the example script Measure-VectorSimilarity written by Lee Holmes, performs a cosine similarity calculation\nand then ranks each row in the datatable. # Sample function to calculate cosine similarity\nfunction Get-CosineSimilarity ([float[]]$vector1, [float[]]$vector2) {\n    $dot = 0\n    $mag1 = 0\n    $mag2 = 0\n\n    $allkeys = 0..($vector1.Length-1)\n\n    foreach ($key in $allkeys) {\n        $dot  += $vector1[$key]  * $vector2[$key]\n        $mag1 += ($vector1[$key] * $vector1[$key])\n        $mag2 += ($vector2[$key] * $vector2[$key])\n    }\n\n    $mag1 = [Math]::Sqrt($mag1)\n    $mag2 = [Math]::Sqrt($mag2)\n\n    return [Math]::Round($dot / ($mag1 * $mag2), 3)\n} The commands in the next example loop through all rows in $Datatable and calculate the cosine\nsimilarity to the search string. The results are sorted and the top three results are stored in a\nvariable named $topThree . The example does not return output. # Calculate cosine similarity for each row and select the top 3\n$topThree = $Datatable | ForEach-Object {\n    [PSCustomObject]@{\n        title = $_.title\n        similarity = Get-CosineSimilarity $_.vectors.split(',') $searchVectors.split(',')\n    }\n} | Sort-Object -property similarity -descending | Select-Object -First 3 | ForEach-Object {\n    $title = $_.title\n    $Datatable | Where-Object { $_.title -eq $title }\n} Review the output of the $topThree variable, with only title and url properties, in\ngridview. $topThree | Select \"title\", \"uri\" | Out-GridView Output: The $topThree variable contains all the information from the rows in the datatable. For example,\nthe content property contains the original document format. Use [0] to index into the first item\nin the array. $topThree[0].content View the full document (truncated in the output snippet for this page). ---\nexternal help file: Microsoft.PowerShell.Commands.Management.dll-Help.xml\nLocale: en-US\nModule Name: Microsoft.PowerShell.Management\nms.date: 07/03/2023\nonline version: https://learn.microsoft.com/powershell/module/microsoft.powershell.management/get-process?view=powershell-7.4&WT.mc_id=ps-gethelp\nschema: 2.0.0\ntitle: Get-Process\n---\n\n# Get-Process\n\n## SYNOPSIS\nGets the processes that are running on the local computer.\n\n## SYNTAX\n\n### Name (Default)\n\nGet-Process [[-Name] <String[]>] [-Module] [-FileVersionInfo] [<CommonParameters>]\n# truncated example Finally, rather than regenerate the embeddings every time you need to query the dataset, you can\nstore the data to disk and recall it in the future. The WriteXML() and ReadXML() methods of DataTable object types in the next example simplify the process. The schema of the XML file\nrequires the datatable to have a TableName . Replace <YOUR-FULL-FILE-PATH> with the full path where you would like to write and read the XML\nfile. The path should end with .xml . # Set DataTable name\n$Datatable.TableName = \"MyDataTable\"\n\n# Writing DataTable to XML\n$Datatable.WriteXml(\"<YOUR-FULL-FILE-PATH>\", [System.Data.XmlWriteMode]::WriteSchema)\n\n# Reading XML back to DataTable\n$newDatatable = New-Object System.Data.DataTable\n$newDatatable.ReadXml(\"<YOUR-FULL-FILE-PATH>\") As you reuse the data, you need to get the vectors of each new search string (but not\nthe entire datatable). As a learning exercise, try creating a PowerShell script to automate the Invoke-RestMethod command with the search string as a parameter. Reference link definitions",
            "html": "\n<ul>\n<li><p>An Azure subscription - <a data-linktype=\"external\" href=\"https://azure.microsoft.com/free/cognitive-services?azure-portal=true\">Create one for free</a></p>\n</li>\n<li><p>An Azure OpenAI resource with the <strong>text-embedding-ada-002 (Version 2)</strong> model deployed.</p>\n<p>This model is currently only available in <a data-linktype=\"relative-path\" href=\"../concepts/models#model-summary-table-and-region-availability\">certain regions</a>.  If you don't have a resource\nthe process of creating one is documented in our <a data-linktype=\"relative-path\" href=\"../how-to/create-resource\">resource deployment guide</a>.</p>\n</li>\n<li><p><a data-linktype=\"external\" href=\"https://aka.ms/install-powershell\">PowerShell 7.4</a></p>\n</li>\n</ul>\n<div class=\"NOTE\">\n<p>Note</p>\n<p>Many examples in this tutorial re-use variables from step-to-step. Keep the same terminal session\nopen throughout. If variables you set in a previous step are lost due to closing the terminal,\nyou must begin again from the start.</p>\n</div>\n<h3 id=\"retrieve-key-and-endpoint\">Retrieve key and endpoint</h3>\n<p>To successfully make a call against Azure OpenAI, you need an <strong>endpoint</strong> and a <strong>key</strong>.</p>\n<table>\n<thead>\n<tr>\n<th>Variable name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ENDPOINT</code></td>\n<td>The service endpoint can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the <strong>Deployments</strong> page in Azure AI Foundry portal. An example endpoint is: <code>https://docs-test-001.openai.azure.com/</code>.</td>\n</tr>\n<tr>\n<td><code>API-KEY</code></td>\n<td>This value can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. You can use either <code>KEY1</code> or <code>KEY2</code>.</td>\n</tr>\n</tbody>\n</table>\n<p>Go to your resource in the Azure portal. The <strong>Keys &amp; Endpoint</strong> section can be found in the <strong>Resource Management</strong> section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either <code>KEY1</code> or <code>KEY2</code>. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.</p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/quickstarts/endpoint.png#lightbox\">\n<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>\n</a>\n</span>\n</p>\n<h3 id=\"environment-variables\">Environment variables</h3>\n<p>Create and assign persistent environment variables for your key and endpoint.</p>\n<div class=\"IMPORTANT\">\n<p>Important</p>\n<p>Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see <a data-linktype=\"absolute-path\" href=\"/en-us/azure/key-vault/general/apps-api-keys-secrets\">API keys with Azure Key Vault</a>.</p>\n<p>For more information about AI services security, see <a data-linktype=\"absolute-path\" href=\"/en-us/azure/ai-services/authentication\">Authenticate requests to Azure AI services</a>.</p>\n</div>\n<div class=\"tabGroup\" id=\"tabgroup_1\">\n<ul role=\"tablist\">\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_1_command-line\" aria-selected=\"true\" data-linktype=\"self-bookmark\" data-tab=\"command-line\" href=\"#tabpanel_1_command-line\" role=\"tab\" tabindex=\"0\">Command Line</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_1_powershell\" data-linktype=\"self-bookmark\" data-tab=\"powershell\" href=\"#tabpanel_1_powershell\" role=\"tab\" tabindex=\"-1\">PowerShell</a>\n</li>\n<li role=\"presentation\">\n<a aria-controls=\"tabpanel_1_bash\" data-linktype=\"self-bookmark\" data-tab=\"bash\" href=\"#tabpanel_1_bash\" role=\"tab\" tabindex=\"-1\">Bash</a>\n</li>\n</ul>\n<section data-tab=\"command-line\" id=\"tabpanel_1_command-line\" role=\"tabpanel\">\n<pre><code class=\"lang-CMD\">setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" \n</code></pre>\n<pre><code class=\"lang-CMD\">setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" \n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"powershell\" hidden=\"hidden\" id=\"tabpanel_1_powershell\" role=\"tabpanel\">\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\">$Env:AZURE_OPENAI_API_KEY = '&lt;YOUR_KEY_VALUE&gt;'\n$Env:AZURE_OPENAI_ENDPOINT = '&lt;YOUR_ENDPOINT&gt;'\n$Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT = '&lt;YOUR_DEPLOYMENT_NAME&gt;'\n</code></pre>\n</section>\n<section aria-hidden=\"true\" data-tab=\"bash\" hidden=\"hidden\" id=\"tabpanel_1_bash\" role=\"tabpanel\">\n<pre><code class=\"lang-Bash\">echo export AZURE_OPENAI_API_KEY=\"&lt;YOUR_KEY_VALUE&gt;\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"&lt;YOUR_ENDPOINT&gt;\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=\"&lt;YOUR_DEPLOYMENT_NAME&gt;\" &gt;&gt; /etc/environment\nsource /etc/environment\n</code></pre>\n</section>\n</div>\n<p>For this tutorial, we use the <a data-linktype=\"absolute-path\" href=\"/en-us/powershell/module/?view=powershell-7.4&amp;preserve-view=true\">PowerShell 7.4 reference documentation</a> as a well-known and safe\nsample dataset. As an alternative, you might choose to explore the <a data-linktype=\"external\" href=\"https://www.microsoft.com/research/tools/\">Microsoft Research tools</a>\nsample datasets.</p>\n<p>Create a folder where you would like to store your project. Set your location to the project\nfolder. Download the dataset to your local machine using the <code>Invoke-WebRequest</code> command and then\nexpand the archive. Last, set your location to the subfolder containing reference information for\nPowerShell version 7.4.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\">New-Item '&lt;FILE-PATH-TO-YOUR-PROJECT&gt;' -Type Directory\nSet-Location '&lt;FILE-PATH-TO-YOUR-PROJECT&gt;'\n\n$DocsUri = 'https://github.com/MicrosoftDocs/PowerShell-Docs/archive/refs/heads/main.zip'\nInvoke-WebRequest $DocsUri -OutFile './PSDocs.zip'\n\nExpand-Archive './PSDocs.zip'\nSet-Location './PSDocs/PowerShell-Docs-main/reference/7.4/'\n</code></pre>\n<p>We're working with a large amount of data in this tutorial, so we use a .NET data table object for\nefficient performance. The datatable has columns <strong>title</strong>, <strong>content</strong>, <strong>prep</strong>,  <strong>uri</strong>,\n<strong>file</strong>, and <strong>vectors</strong>. The <strong>title</strong> column is the <a data-linktype=\"absolute-path\" href=\"/en-us/dotnet/framework/data/adonet/dataset-datatable-dataview/defining-primary-keys\">primary key</a>.</p>\n<p>In the next step, we load the content of each markdown file into the data table. We also use\nPowerShell <code>-match</code> operator to capture known lines of text <code>title:</code> and <code>online version:</code>, and\nstore them in distinct columns. Some of the files don't contain the metadata lines of text, but\nsince they're overview pages and not detailed reference docs, we exclude them from the datatable.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># make sure your location is the project subfolder\n\n$DataTable = New-Object System.Data.DataTable\n\n'title', 'content', 'prep', 'uri', 'file', 'vectors' | ForEach-Object {\n    $DataTable.Columns.Add($_)\n} | Out-Null\n$DataTable.PrimaryKey = $DataTable.Columns['title']\n\n$md = Get-ChildItem -Path . -Include *.md -Recurse\n\n$md | ForEach-Object {\n    $file       = $_.FullName\n    $content    = Get-Content $file\n    $title      = $content | Where-Object { $_ -match 'title: ' }\n    $uri        = $content | Where-Object { $_ -match 'online version: ' }\n    if ($title -and $uri) {\n        $row                = $DataTable.NewRow()\n        $row.title          = $title.ToString().Replace('title: ', '')\n        $row.content        = $content | Out-String\n        $row.prep           = '' # use later in the tutorial\n        $row.uri            = $uri.ToString().Replace('online version: ', '')\n        $row.file           = $file\n        $row.vectors        = '' # use later in the tutorial\n        $Datatable.rows.add($row)\n    }\n}\n</code></pre>\n<p>View the data using the <code>out-gridview</code> command (not available in Cloud Shell).</p>\n<pre><code class=\"lang-powershell\">$Datatable | out-gridview\n</code></pre>\n<p>Output:</p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/initial-datatable.png#lightbox\">\n<img alt=\"Screenshot of the initial DataTable results.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-datatable.png\"/>\n</a>\n</span>\n</p>\n<p>Next perform some light data cleaning by removing extra characters, empty space, and other document\nnotations, to prepare the data for tokenization. The sample function <code>Invoke-DocPrep</code> demonstrates\nhow to use the PowerShell <code>-replace</code> operator to iterate through a list of characters you would like\nto remove from the content.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># sample demonstrates how to use `-replace` to remove characters from text content\nfunction Invoke-DocPrep {\nparam(\n    [Parameter(Mandatory = $true, ValueFromPipeline = $true)]\n    [string]$content\n)\n    # tab, line breaks, empty space\n    $replace = @('\\t','\\r\\n','\\n','\\r')\n    # non-UTF8 characters\n    $replace += @('[^\\x00-\\x7F]')\n    # html\n    $replace += @('&lt;table&gt;','&lt;/table&gt;','&lt;tr&gt;','&lt;/tr&gt;','&lt;td&gt;','&lt;/td&gt;')\n    $replace += @('&lt;ul&gt;','&lt;/ul&gt;','&lt;li&gt;','&lt;/li&gt;')\n    $replace += @('&lt;p&gt;','&lt;/p&gt;','&lt;br&gt;')\n    # docs\n    $replace += @('\\*\\*IMPORTANT:\\*\\*','\\*\\*NOTE:\\*\\*')\n    $replace += @('&lt;!','no-loc ','text=')\n    $replace += @('&lt;--','--&gt;','---','--',':::')\n    # markdown\n    $replace += @('###','##','#','```')\n    $replace | ForEach-Object {\n        $content = $content -replace $_, ' ' -replace '  ',' '\n    }\n    return $content\n}\n</code></pre>\n<p>After you create the <code>Invoke-DocPrep</code> function, use the <code>ForEach-Object</code> command to store prepared\ncontent in the <strong>prep</strong> column, for all rows in the datatable. We're using a new column so the\noriginal formatting is available if we would like to retrieve it later.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\">$Datatable.rows | ForEach-Object { $_.prep = Invoke-DocPrep $_.content }\n</code></pre>\n<p>View the datatable again to see the change.</p>\n<pre><code class=\"lang-powershell\">$Datatable | out-gridview\n</code></pre>\n<p>When we pass the documents to the embeddings model, it encodes the documents into tokens and then\nreturns a series of floating point numbers to use in a <a data-linktype=\"relative-path\" href=\"../concepts/understand-embeddings#cosine-similarity\">cosine similarity</a> search. These\nembeddings can be stored locally or in a service such as <a data-linktype=\"absolute-path\" href=\"/en-us/azure/search/vector-search-overview\">Vector Search in Azure AI Search</a>.\nEach document has its own corresponding embedding vector in the new <strong>vectors</strong> column.</p>\n<p>The next example loops through each row in the datatable, retrieves the vectors for the\npreprocessed content, and stores them to the <strong>vectors</strong> column. The OpenAI service throttles\nfrequent requests, so the example includes an <strong>exponential back-off</strong> as suggested by the\n<a data-linktype=\"external\" href=\"https://platform.openai.com/docs/guides/rate-limits/error-mitigation\">documentation</a>.</p>\n<p>After the script completes, each row should have a comma-delimited list of 1536 vectors for each\ndocument. If an error occurs and the status code is <code>400</code>, the file path, title, and error code\nare added to a variable named <code>$errorDocs</code> for troubleshooting. The most common error occurs when\nthe token count is more than the prompt limit for the model.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># Azure OpenAI metadata variables\n$openai = @{\n    api_key     = $Env:AZURE_OPENAI_API_KEY \n    api_base    = $Env:AZURE_OPENAI_ENDPOINT # should look like 'https://&lt;YOUR_RESOURCE_NAME&gt;.openai.azure.com/'\n    api_version = '2024-02-01' # may change in the future\n    name        = $Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT # custom name you chose for your deployment\n}\n\n$headers = [ordered]@{\n    'api-key' = $openai.api_key\n}\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$Datatable | ForEach-Object {\n    $doc = $_\n\n    $body = [ordered]@{\n        input = $doc.prep\n    } | ConvertTo-Json\n\n    $retryCount = 0\n    $maxRetries = 10\n    $delay      = 1\n    $docErrors = @()\n\n    do {\n        try {\n            $params = @{\n                Uri         = $url\n                Headers     = $headers\n                Body        = $body\n                Method      = 'Post'\n                ContentType = 'application/json'\n            }\n            $response = Invoke-RestMethod @params\n            $Datatable.rows.find($doc.title).vectors = $response.data.embedding -join ','\n            break\n        } catch {\n            if ($_.Exception.Response.StatusCode -eq 429) {\n                $retryCount++\n                [int]$retryAfter = $_.Exception.Response.Headers |\n                    Where-Object key -eq 'Retry-After' |\n                    Select-Object -ExpandProperty Value\n\n                # Use delay from error header\n                if ($delay -lt $retryAfter) { $delay = $retryAfter++ }\n                Start-Sleep -Seconds $delay\n                # Exponential back-off\n                $delay = [math]::min($delay * 1.5, 300)\n            } elseif ($_.Exception.Response.StatusCode -eq 400) {\n                if ($docErrors.file -notcontains $doc.file) {\n                    $docErrors += [ordered]@{\n                        error   = $_.exception.ErrorDetails.Message | ForEach-Object error | ForEach-Object message\n                        file    = $doc.file\n                        title   = $doc.title\n                    }\n                }\n            } else {\n                throw\n            }\n        }\n    } while ($retryCount -lt $maxRetries)\n}\nif (0 -lt $docErrors.count) {\n    Write-Host \"$($docErrors.count) documents encountered known errors such as too many tokens.`nReview the `$docErrors variable for details.\"\n}\n</code></pre>\n<p>You now have a local in-memory database table of PowerShell 7.4 reference docs.</p>\n<p>Based on a search string, we need to calculate another set of vectors so PowerShell can rank each\ndocument by similarity.</p>\n<p>In the next example, vectors are retrieved for the search string <code>get a list of running processes</code>.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\">$searchText = \"get a list of running processes\"\n\n$body = [ordered]@{\n    input = $searchText\n} | ConvertTo-Json\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$params = @{\n    Uri         = $url\n    Headers     = $headers\n    Body        = $body\n    Method      = 'Post'\n    ContentType = 'application/json'\n}\n$response = Invoke-RestMethod @params\n$searchVectors = $response.data.embedding -join ','\n</code></pre>\n<p>Finally, the next sample function, which borrows an example from the example script\n<a data-linktype=\"external\" href=\"https://www.powershellgallery.com/packages/Measure-VectorSimilarity/\">Measure-VectorSimilarity</a> written by Lee Holmes, performs a <strong>cosine similarity</strong> calculation\nand then ranks each row in the datatable.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># Sample function to calculate cosine similarity\nfunction Get-CosineSimilarity ([float[]]$vector1, [float[]]$vector2) {\n    $dot = 0\n    $mag1 = 0\n    $mag2 = 0\n\n    $allkeys = 0..($vector1.Length-1)\n\n    foreach ($key in $allkeys) {\n        $dot  += $vector1[$key]  * $vector2[$key]\n        $mag1 += ($vector1[$key] * $vector1[$key])\n        $mag2 += ($vector2[$key] * $vector2[$key])\n    }\n\n    $mag1 = [Math]::Sqrt($mag1)\n    $mag2 = [Math]::Sqrt($mag2)\n\n    return [Math]::Round($dot / ($mag1 * $mag2), 3)\n}\n</code></pre>\n<p>The commands in the next example loop through all rows in <code>$Datatable</code> and calculate the cosine\nsimilarity to the search string. The results are sorted and the top three results are stored in a\nvariable named <code>$topThree</code>. The example does not return output.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># Calculate cosine similarity for each row and select the top 3\n$topThree = $Datatable | ForEach-Object {\n    [PSCustomObject]@{\n        title = $_.title\n        similarity = Get-CosineSimilarity $_.vectors.split(',') $searchVectors.split(',')\n    }\n} | Sort-Object -property similarity -descending | Select-Object -First 3 | ForEach-Object {\n    $title = $_.title\n    $Datatable | Where-Object { $_.title -eq $title }\n}\n</code></pre>\n<p>Review the output of the <code>$topThree</code> variable, with only <strong>title</strong> and <strong>url</strong> properties, in\ngridview.</p>\n<pre><code class=\"lang-powershell\">$topThree | Select \"title\", \"uri\" | Out-GridView\n</code></pre>\n<p><strong>Output:</strong></p>\n<p><span class=\"mx-imgBorder\">\n<a data-linktype=\"relative-path\" href=\"../media/tutorials/query-result-powershell.png#lightbox\">\n<img alt=\"Screenshot of the formatted results once the search query finishes.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result-powershell.png\"/>\n</a>\n</span>\n</p>\n<p>The <code>$topThree</code> variable contains all the information from the rows in the datatable. For example,\nthe <strong>content</strong> property contains the original document format. Use <code>[0]</code> to index into the first item\nin the array.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\">$topThree[0].content\n</code></pre>\n<p>View the full document (truncated in the output snippet for this page).</p>\n<pre><code class=\"lang-Output\">---\nexternal help file: Microsoft.PowerShell.Commands.Management.dll-Help.xml\nLocale: en-US\nModule Name: Microsoft.PowerShell.Management\nms.date: 07/03/2023\nonline version: https://learn.microsoft.com/powershell/module/microsoft.powershell.management/get-process?view=powershell-7.4&amp;WT.mc_id=ps-gethelp\nschema: 2.0.0\ntitle: Get-Process\n---\n\n# Get-Process\n\n## SYNOPSIS\nGets the processes that are running on the local computer.\n\n## SYNTAX\n\n### Name (Default)\n\nGet-Process [[-Name] &lt;String[]&gt;] [-Module] [-FileVersionInfo] [&lt;CommonParameters&gt;]\n# truncated example\n</code></pre>\n<p>Finally, rather than regenerate the embeddings every time you need to query the dataset, you can\nstore the data to disk and recall it in the future. The <code>WriteXML()</code> and <code>ReadXML()</code> methods of\n<strong>DataTable</strong> object types in the next example simplify the process. The schema of the XML file\nrequires the datatable to have a <strong>TableName</strong>.</p>\n<p>Replace <code>&lt;YOUR-FULL-FILE-PATH&gt;</code> with the full path where you would like to write and read the XML\nfile. The path should end with <code>.xml</code>.</p>\n<pre><code class=\"lang-powershell\" data-interactive=\"powershell\"># Set DataTable name\n$Datatable.TableName = \"MyDataTable\"\n\n# Writing DataTable to XML\n$Datatable.WriteXml(\"&lt;YOUR-FULL-FILE-PATH&gt;\", [System.Data.XmlWriteMode]::WriteSchema)\n\n# Reading XML back to DataTable\n$newDatatable = New-Object System.Data.DataTable\n$newDatatable.ReadXml(\"&lt;YOUR-FULL-FILE-PATH&gt;\")\n</code></pre>\n<p>As you reuse the data, you need to get the vectors of each new search string (but not\nthe entire datatable). As a learning exercise, try creating a PowerShell script to automate the\n<code>Invoke-RestMethod</code> command with the search string as a parameter.</p>\n Reference link definitions \n",
            "images": [
                "<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>",
                "<img alt=\"Screenshot of the initial DataTable results.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-datatable.png\"/>",
                "<img alt=\"Screenshot of the formatted results once the search query finishes.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result-powershell.png\"/>"
            ],
            "img_alt": [
                "<img alt=\"Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint and access keys location circled in red.\" data-linktype=\"relative-path\" src=\"../media/quickstarts/endpoint.png\"/>",
                "<img alt=\"Screenshot of the initial DataTable results.\" data-linktype=\"relative-path\" src=\"../media/tutorials/initial-datatable.png\"/>",
                "<img alt=\"Screenshot of the formatted results once the search query finishes.\" data-linktype=\"relative-path\" src=\"../media/tutorials/query-result-powershell.png\"/>"
            ],
            "tables": [
                "<table>\n<thead>\n<tr>\n<th>Variable name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ENDPOINT</code></td>\n<td>The service endpoint can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the <strong>Deployments</strong> page in Azure AI Foundry portal. An example endpoint is: <code>https://docs-test-001.openai.azure.com/</code>.</td>\n</tr>\n<tr>\n<td><code>API-KEY</code></td>\n<td>This value can be found in the <strong>Keys &amp; Endpoint</strong> section when examining your resource from the Azure portal. You can use either <code>KEY1</code> or <code>KEY2</code>.</td>\n</tr>\n</tbody>\n</table>"
            ],
            "codes": [
                "<code>ENDPOINT</code>",
                "<code>https://docs-test-001.openai.azure.com/</code>",
                "<code>API-KEY</code>",
                "<code>KEY1</code>",
                "<code>KEY2</code>",
                "<code>KEY1</code>",
                "<code>KEY2</code>",
                "<code class=\"lang-CMD\">setx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\" \n</code>",
                "<code class=\"lang-CMD\">setx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\" \n</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\">$Env:AZURE_OPENAI_API_KEY = '&lt;YOUR_KEY_VALUE&gt;'\n$Env:AZURE_OPENAI_ENDPOINT = '&lt;YOUR_ENDPOINT&gt;'\n$Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT = '&lt;YOUR_DEPLOYMENT_NAME&gt;'\n</code>",
                "<code class=\"lang-Bash\">echo export AZURE_OPENAI_API_KEY=\"&lt;YOUR_KEY_VALUE&gt;\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_ENDPOINT=\"&lt;YOUR_ENDPOINT&gt;\" &gt;&gt; /etc/environment\necho export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=\"&lt;YOUR_DEPLOYMENT_NAME&gt;\" &gt;&gt; /etc/environment\nsource /etc/environment\n</code>",
                "<code>Invoke-WebRequest</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\">New-Item '&lt;FILE-PATH-TO-YOUR-PROJECT&gt;' -Type Directory\nSet-Location '&lt;FILE-PATH-TO-YOUR-PROJECT&gt;'\n\n$DocsUri = 'https://github.com/MicrosoftDocs/PowerShell-Docs/archive/refs/heads/main.zip'\nInvoke-WebRequest $DocsUri -OutFile './PSDocs.zip'\n\nExpand-Archive './PSDocs.zip'\nSet-Location './PSDocs/PowerShell-Docs-main/reference/7.4/'\n</code>",
                "<code>-match</code>",
                "<code>title:</code>",
                "<code>online version:</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># make sure your location is the project subfolder\n\n$DataTable = New-Object System.Data.DataTable\n\n'title', 'content', 'prep', 'uri', 'file', 'vectors' | ForEach-Object {\n    $DataTable.Columns.Add($_)\n} | Out-Null\n$DataTable.PrimaryKey = $DataTable.Columns['title']\n\n$md = Get-ChildItem -Path . -Include *.md -Recurse\n\n$md | ForEach-Object {\n    $file       = $_.FullName\n    $content    = Get-Content $file\n    $title      = $content | Where-Object { $_ -match 'title: ' }\n    $uri        = $content | Where-Object { $_ -match 'online version: ' }\n    if ($title -and $uri) {\n        $row                = $DataTable.NewRow()\n        $row.title          = $title.ToString().Replace('title: ', '')\n        $row.content        = $content | Out-String\n        $row.prep           = '' # use later in the tutorial\n        $row.uri            = $uri.ToString().Replace('online version: ', '')\n        $row.file           = $file\n        $row.vectors        = '' # use later in the tutorial\n        $Datatable.rows.add($row)\n    }\n}\n</code>",
                "<code>out-gridview</code>",
                "<code class=\"lang-powershell\">$Datatable | out-gridview\n</code>",
                "<code>Invoke-DocPrep</code>",
                "<code>-replace</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># sample demonstrates how to use `-replace` to remove characters from text content\nfunction Invoke-DocPrep {\nparam(\n    [Parameter(Mandatory = $true, ValueFromPipeline = $true)]\n    [string]$content\n)\n    # tab, line breaks, empty space\n    $replace = @('\\t','\\r\\n','\\n','\\r')\n    # non-UTF8 characters\n    $replace += @('[^\\x00-\\x7F]')\n    # html\n    $replace += @('&lt;table&gt;','&lt;/table&gt;','&lt;tr&gt;','&lt;/tr&gt;','&lt;td&gt;','&lt;/td&gt;')\n    $replace += @('&lt;ul&gt;','&lt;/ul&gt;','&lt;li&gt;','&lt;/li&gt;')\n    $replace += @('&lt;p&gt;','&lt;/p&gt;','&lt;br&gt;')\n    # docs\n    $replace += @('\\*\\*IMPORTANT:\\*\\*','\\*\\*NOTE:\\*\\*')\n    $replace += @('&lt;!','no-loc ','text=')\n    $replace += @('&lt;--','--&gt;','---','--',':::')\n    # markdown\n    $replace += @('###','##','#','```')\n    $replace | ForEach-Object {\n        $content = $content -replace $_, ' ' -replace '  ',' '\n    }\n    return $content\n}\n</code>",
                "<code>Invoke-DocPrep</code>",
                "<code>ForEach-Object</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\">$Datatable.rows | ForEach-Object { $_.prep = Invoke-DocPrep $_.content }\n</code>",
                "<code class=\"lang-powershell\">$Datatable | out-gridview\n</code>",
                "<code>400</code>",
                "<code>$errorDocs</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># Azure OpenAI metadata variables\n$openai = @{\n    api_key     = $Env:AZURE_OPENAI_API_KEY \n    api_base    = $Env:AZURE_OPENAI_ENDPOINT # should look like 'https://&lt;YOUR_RESOURCE_NAME&gt;.openai.azure.com/'\n    api_version = '2024-02-01' # may change in the future\n    name        = $Env:AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT # custom name you chose for your deployment\n}\n\n$headers = [ordered]@{\n    'api-key' = $openai.api_key\n}\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$Datatable | ForEach-Object {\n    $doc = $_\n\n    $body = [ordered]@{\n        input = $doc.prep\n    } | ConvertTo-Json\n\n    $retryCount = 0\n    $maxRetries = 10\n    $delay      = 1\n    $docErrors = @()\n\n    do {\n        try {\n            $params = @{\n                Uri         = $url\n                Headers     = $headers\n                Body        = $body\n                Method      = 'Post'\n                ContentType = 'application/json'\n            }\n            $response = Invoke-RestMethod @params\n            $Datatable.rows.find($doc.title).vectors = $response.data.embedding -join ','\n            break\n        } catch {\n            if ($_.Exception.Response.StatusCode -eq 429) {\n                $retryCount++\n                [int]$retryAfter = $_.Exception.Response.Headers |\n                    Where-Object key -eq 'Retry-After' |\n                    Select-Object -ExpandProperty Value\n\n                # Use delay from error header\n                if ($delay -lt $retryAfter) { $delay = $retryAfter++ }\n                Start-Sleep -Seconds $delay\n                # Exponential back-off\n                $delay = [math]::min($delay * 1.5, 300)\n            } elseif ($_.Exception.Response.StatusCode -eq 400) {\n                if ($docErrors.file -notcontains $doc.file) {\n                    $docErrors += [ordered]@{\n                        error   = $_.exception.ErrorDetails.Message | ForEach-Object error | ForEach-Object message\n                        file    = $doc.file\n                        title   = $doc.title\n                    }\n                }\n            } else {\n                throw\n            }\n        }\n    } while ($retryCount -lt $maxRetries)\n}\nif (0 -lt $docErrors.count) {\n    Write-Host \"$($docErrors.count) documents encountered known errors such as too many tokens.`nReview the `$docErrors variable for details.\"\n}\n</code>",
                "<code>get a list of running processes</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\">$searchText = \"get a list of running processes\"\n\n$body = [ordered]@{\n    input = $searchText\n} | ConvertTo-Json\n\n$url = \"$($openai.api_base)/openai/deployments/$($openai.name)/embeddings?api-version=$($openai.api_version)\"\n\n$params = @{\n    Uri         = $url\n    Headers     = $headers\n    Body        = $body\n    Method      = 'Post'\n    ContentType = 'application/json'\n}\n$response = Invoke-RestMethod @params\n$searchVectors = $response.data.embedding -join ','\n</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># Sample function to calculate cosine similarity\nfunction Get-CosineSimilarity ([float[]]$vector1, [float[]]$vector2) {\n    $dot = 0\n    $mag1 = 0\n    $mag2 = 0\n\n    $allkeys = 0..($vector1.Length-1)\n\n    foreach ($key in $allkeys) {\n        $dot  += $vector1[$key]  * $vector2[$key]\n        $mag1 += ($vector1[$key] * $vector1[$key])\n        $mag2 += ($vector2[$key] * $vector2[$key])\n    }\n\n    $mag1 = [Math]::Sqrt($mag1)\n    $mag2 = [Math]::Sqrt($mag2)\n\n    return [Math]::Round($dot / ($mag1 * $mag2), 3)\n}\n</code>",
                "<code>$Datatable</code>",
                "<code>$topThree</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># Calculate cosine similarity for each row and select the top 3\n$topThree = $Datatable | ForEach-Object {\n    [PSCustomObject]@{\n        title = $_.title\n        similarity = Get-CosineSimilarity $_.vectors.split(',') $searchVectors.split(',')\n    }\n} | Sort-Object -property similarity -descending | Select-Object -First 3 | ForEach-Object {\n    $title = $_.title\n    $Datatable | Where-Object { $_.title -eq $title }\n}\n</code>",
                "<code>$topThree</code>",
                "<code class=\"lang-powershell\">$topThree | Select \"title\", \"uri\" | Out-GridView\n</code>",
                "<code>$topThree</code>",
                "<code>[0]</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\">$topThree[0].content\n</code>",
                "<code class=\"lang-Output\">---\nexternal help file: Microsoft.PowerShell.Commands.Management.dll-Help.xml\nLocale: en-US\nModule Name: Microsoft.PowerShell.Management\nms.date: 07/03/2023\nonline version: https://learn.microsoft.com/powershell/module/microsoft.powershell.management/get-process?view=powershell-7.4&amp;WT.mc_id=ps-gethelp\nschema: 2.0.0\ntitle: Get-Process\n---\n\n# Get-Process\n\n## SYNOPSIS\nGets the processes that are running on the local computer.\n\n## SYNTAX\n\n### Name (Default)\n\nGet-Process [[-Name] &lt;String[]&gt;] [-Module] [-FileVersionInfo] [&lt;CommonParameters&gt;]\n# truncated example\n</code>",
                "<code>WriteXML()</code>",
                "<code>ReadXML()</code>",
                "<code>&lt;YOUR-FULL-FILE-PATH&gt;</code>",
                "<code>.xml</code>",
                "<code class=\"lang-powershell\" data-interactive=\"powershell\"># Set DataTable name\n$Datatable.TableName = \"MyDataTable\"\n\n# Writing DataTable to XML\n$Datatable.WriteXml(\"&lt;YOUR-FULL-FILE-PATH&gt;\", [System.Data.XmlWriteMode]::WriteSchema)\n\n# Reading XML back to DataTable\n$newDatatable = New-Object System.Data.DataTable\n$newDatatable.ReadXml(\"&lt;YOUR-FULL-FILE-PATH&gt;\")\n</code>",
                "<code>Invoke-RestMethod</code>"
            ]
        }
    },
    {
        "Clean up resources": {
            "tag": "h2",
            "text": "If you created an Azure OpenAI resource solely for completing this tutorial and want to clean up and remove an Azure OpenAI resource, you'll need to delete your deployed models, and then delete the resource or associated resource group if it's dedicated to your test resource. Deleting the resource group also deletes any other resources associated with it. Azure portal Azure CLI",
            "html": "\n<p>If you created an Azure OpenAI resource solely for completing this tutorial and want to clean up and remove an Azure OpenAI resource, you'll need to delete your deployed models, and then delete the resource or associated resource group if it's dedicated to your test resource. Deleting the resource group also deletes any other resources associated with it.</p>\n<ul>\n<li><a data-linktype=\"relative-path\" href=\"../../multi-service-resource?pivots=azportal#clean-up-resources\">Azure portal</a></li>\n<li><a data-linktype=\"relative-path\" href=\"../../multi-service-resource?pivots=azcli#clean-up-resources\">Azure CLI</a></li>\n</ul>\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "Next steps": {
            "tag": "h2",
            "text": "Learn more about Azure OpenAI's models: Azure OpenAI models Store your embeddings and perform vector (similarity) search using your choice of Azure service: Azure AI Search Azure SQL Database Azure Cosmos DB for MongoDB vCore Azure SQL Database Azure Cosmos DB for NoSQL Azure Cosmos DB for PostgreSQL Azure Cache for Redis",
            "html": "\n<p>Learn more about Azure OpenAI's models:</p>\n<div class=\"nextstepaction\">\n<p><a data-linktype=\"relative-path\" href=\"../concepts/models\">Azure OpenAI models</a></p>\n</div>\n<ul>\n<li>Store your embeddings and perform vector (similarity) search using your choice of Azure service:\n<ul>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/search/vector-search-overview\">Azure AI Search</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/azure-sql/database/ai-artificial-intelligence-intelligent-applications?view=azuresql&amp;preserve-view=true#vector-search\">Azure SQL Database</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/cosmos-db/mongodb/vcore/vector-search\">Azure Cosmos DB for MongoDB vCore</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/azure-sql/database/ai-artificial-intelligence-intelligent-applications?view=azuresql&amp;preserve-view=true#vector-search\">Azure SQL Database</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/cosmos-db/vector-search\">Azure Cosmos DB for NoSQL</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/cosmos-db/postgresql/howto-use-pgvector\">Azure Cosmos DB for PostgreSQL</a></li>\n<li><a data-linktype=\"absolute-path\" href=\"/en-us/azure/azure-cache-for-redis/cache-tutorial-vector-similarity\">Azure Cache for Redis</a></li>\n</ul>\n</li>\n</ul>\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "Feedback": {
            "tag": "h2",
            "text": "Was this page helpful? Yes No Provide product feedback | Get help at Microsoft Q&A",
            "html": "\n<div class=\"display-flex flex-wrap-wrap align-items-center\">\n<p class=\"font-weight-semibold margin-xxs margin-left-none\">\n\t\t\t\t\tWas this page helpful?\n\t\t\t\t</p>\n<div class=\"buttons\">\n<button aria-pressed=\"false\" class=\"thumb-rating-button like button button-primary button-sm\" data-bi-name=\"button-rating-yes\" data-binary-rating-response=\"rating-yes\" data-test-id=\"footer-rating-yes\" title=\"This article is helpful\" type=\"button\">\n<span aria-hidden=\"true\" class=\"icon\">\n<span class=\"docon docon-like\"></span>\n</span>\n<span>Yes</span>\n</button>\n<button aria-pressed=\"false\" class=\"thumb-rating-button dislike button button-primary button-sm\" data-bi-name=\"button-rating-no\" data-binary-rating-response=\"rating-no\" data-test-id=\"footer-rating-no\" title=\"This article is not helpful\" type=\"button\">\n<span aria-hidden=\"true\" class=\"icon\">\n<span class=\"docon docon-dislike\"></span>\n</span>\n<span>No</span>\n</button>\n</div>\n</div>\n<div class=\"display-flex flex-wrap-wrap gap-xxs margin-top-xxs\">\n<a class=\"has-external-link-indicator\" data-bi-name=\"product-feedback\" href=\"https://feedback.azure.com/d365community/forum/79b1327d-d925-ec11-b6e6-000d3a4f06a4\">\n<span>Provide product feedback</span>\n</a>\n<span aria-hidden=\"true\">|</span>\n<a class=\"has-external-link-indicator\" data-bi-name=\"get-help-at-qna\" href=\"https://learn.microsoft.com/answers/tags/387/azure-openai\">\n<span>Get help at Microsoft Q&amp;A</span>\n</a>\n</div>\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    },
    {
        "Additional resources": {
            "tag": "h2",
            "text": "",
            "html": "\n<section class=\"\" data-bi-name=\"recommendations\" hidden=\"\" id=\"right-rail-recommendations-mobile\"></section>\n<section class=\"\" data-bi-name=\"learning-resource-card\" hidden=\"\" id=\"right-rail-training-mobile\"></section>\n<section class=\"\" data-bi-name=\"events-card\" hidden=\"\" id=\"right-rail-events-mobile\"></section>\n<section class=\"margin-top-xxs\" data-bi-name=\"qna-link-card\" hidden=\"\" id=\"right-rail-qna-mobile\"></section>\n",
            "images": [],
            "img_alt": [],
            "tables": [],
            "codes": []
        }
    }
]